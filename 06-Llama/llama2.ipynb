{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlaMa2\n",
    "\n",
    "参考：https://www.philschmid.de/instruction-tune-llama-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6'\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from random import randrange\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\"/data1/zhengnanyan/code/transformers-code-master/06-LLM/dataset/databricks/databricks-dolly-15k\")\n",
    "\n",
    "# print(f\"dataset size: {len(dataset)}\")\n",
    "# print(dataset[randrange(len(dataset))])\n",
    "# # dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从本地加载数据集\n",
    "\n",
    "用path指定数据集格式\n",
    "\n",
    "json格式，path=\"json\"\n",
    "\n",
    "csv格式， path=\"csv\"\n",
    "\n",
    "纯文本格式, path=\"text\"\n",
    "\n",
    "dataframe格式， path=\"panda\"\n",
    "\n",
    "图片，path=\"imagefolder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# code_data = pd.read_excel(\"/data1/zhengnanyan/code/transformers-code-master/06-LLM/dataset/code-generation/all_data.xlsx\")\n",
    "# code_data.fillna('==', inplace=True)\n",
    "# code_data.to_csv(\"/data1/zhengnanyan/code/transformers-code-master/06-LLM/dataset/code-generation/all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"/data1/zhengnanyan/code/transformers-code-master/06-LLM/dataset/code-generation/all_data.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将结构化数据转化为instruction指令型数据\n",
    "\n",
    "To instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a formatting_function that takes a sample and returns a string with our format instruction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_instruction(sample):\n",
    "#     return f\"\"\"### Instruction:\n",
    "# Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
    "\n",
    "# ### Input:\n",
    "# {sample['description']}\n",
    "\n",
    "# ### Response:\n",
    "# {sample['keywords']}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that \n",
    "provides further context. Write a response that appropriately completes \n",
    "the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['title']}\n",
    "\n",
    "### Input:\n",
    "{sample['keywords']}\n",
    "\n",
    "### Response:\n",
    "{sample['description']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that \n",
      "provides further context. Write a response that appropriately completes \n",
      "the request.\n",
      "\n",
      "### Instruction:\n",
      "Clear list\n",
      "\n",
      "### Input:\n",
      "clear\n",
      "\n",
      "### Response:\n",
      "Clear the list and print it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2909500557864b0e960baf440d159436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/data1/zhengnanyan/huggingface/modelscope/Llama-2-7b-ms\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map='auto')\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_dir：模型训练输出的目录，包括保存模型和其他训练输出。\n",
    "\n",
    "overwrite_output_dir：如果设置为True，将覆盖输出目录中的内容。\n",
    "\n",
    "num_train_epochs：训练的轮数（epochs）。\n",
    "\n",
    "per_device_train_batch_size：每个训练设备上的批量大小。\n",
    "\n",
    "per_device_eval_batch_size：每个评估设备上的批量大小。\n",
    "\n",
    "save_steps：定义多少个更新步骤保存一次模型。\n",
    "\n",
    "save_total_limit：保存的最大模型数量，用于控制磁盘空间占用。\n",
    "\n",
    "evaluation_strategy：评估策略，可选值有\"steps\"（每隔一定步骤评估）和\"epoch\"（每个epoch评估一次）。\n",
    "\n",
    "logging_steps：定义多少个更新步骤打印一次训练日志。\n",
    "\n",
    "logging_dir：日志输出的目录。\n",
    "\n",
    "do_train：是否进行训练。\n",
    "\n",
    "do_eval：是否进行评估。\n",
    "\n",
    "learning_rate：初始学习率。\n",
    "\n",
    "weight_decay：权重衰减（L2正则化）。\n",
    "\n",
    "gradient_accumulation_steps：梯度累积步骤，用于更大的批次训练。\n",
    "\n",
    "seed：随机数种子，用于可复现性。\n",
    "\n",
    "report_to：定义输出的报告格式，例如\"tensorboard\"、“wandb”（Weights & Biases）等。\n",
    "\n",
    "disable_tqdm：是否禁用tqdm进度条。\n",
    "\n",
    "load_best_model_at_end：训练结束时是否加载最佳模型。\n",
    "\n",
    "metric_for_best_model：用于选择最佳模型的指标。\n",
    "\n",
    "————————————————\n",
    "\n",
    "原文链接：https://blog.csdn.net/weixin_43731005/article/details/132117538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama2-code\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6 ,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # disable_tqdm=True, # disable tqdm since with packing values are in correct\n",
    "    save_steps=20,\n",
    "    # load_best_model_at_end=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a3e9563ee04b40b3940221159f01eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.712400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data1/zhengnanyan/huggingface/modelscope/Llama-2-7b-ms - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data1/zhengnanyan/huggingface/modelscope/Llama-2-7b-ms - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /data1/zhengnanyan/huggingface/modelscope/Llama-2-7b-ms - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21, training_loss=1.1310061840783983, metrics={'train_runtime': 32.7993, 'train_samples_per_second': 7.226, 'train_steps_per_second': 0.64, 'total_flos': 2417499398209536.0, 'train_loss': 1.1310061840783983, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 256 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81ec32babaa4087a72726fff626750f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data1/zhengnanyan/miniconda3/envs/LLMpython39/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "# load base LLM model and tokenizer\n",
    "args.output_dir='/data1/zhengnanyan/code/transformers-code-master/06-LLM/llama2-code/checkpoint-3'\n",
    "\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "from peft import PeftModel\n",
    "\n",
    "'''\n",
    "为什么要像下面那样加载：即先加载预训练模型，再加载我们微调的模型——因为使用lora。\n",
    "lora我们只训练部分参数，多以微调后保存的模型参数无法直接用于加载模型。\n",
    "要把那一部分参数和base model合并\n",
    "'''\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "tokenizer.padding_side='right' # 一定要设置padding_side为right，否则batch大于1时可能不收敛\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_path = \"/data1/zhengnanyan/huggingface/modelscope/Llama-2-7b-ms\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map='auto')\n",
    "\n",
    "p_model = PeftModel.from_pretrained(model, model_id=args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 163,\n",
       " 'title': 'Print numbers',\n",
       " 'keywords': 'print',\n",
       " 'description': 'Print the number 10.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[randrange(len(dataset))]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that \n",
    "provides further context. Write a response that appropriately completes \n",
    "the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['title']}\n",
    "\n",
    "### Input:\n",
    "{sample['keywords']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393, 29871,    13, 16123,  2247,\n",
       "          4340,  3030, 29889, 14350,   263,  2933,   393,  7128,  2486,  1614,\n",
       "          2167, 29871,    13,  1552,  2009, 29889,    13,    13,  2277, 29937,\n",
       "          2799,  4080, 29901,    13, 13463,   403,   975,  1347,    13,    13,\n",
       "          2277, 29937, 10567, 29901,    13,  1454,    13,    13,  2277, 29937,\n",
       "         13291, 29901,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393, 29871,    13, 16123,  2247,\n",
       "          4340,  3030, 29889, 14350,   263,  2933,   393,  7128,  2486,  1614,\n",
       "          2167, 29871,    13,  1552,  2009, 29889,    13,    13,  2277, 29937,\n",
       "          2799,  4080, 29901,    13, 13463,   403,   975,  1347,    13,    13,\n",
       "          2277, 29937, 10567, 29901,    13,  1454,    13,    13,  2277, 29937,\n",
       "         13291, 29901,    13, 29961, 13463,   403,   975,  1347,   850,   991,\n",
       "           597,  1636, 29889, 29893, 29941,   816,  8789, 29889,   510, 29914,\n",
       "          4691, 29914,   999, 29918,  1761, 29918,  1524, 29889,  4692, 29897,\n",
       "            13,    13,  2277, 29937,  2799,  4080, 29901,    13,  3206,   457,\n",
       "           263,  2286,   393,  3743,   385,  6043,    13,    13,  2277, 29937,\n",
       "         10567, 29901,    13,  1753,    13,    13,  2277, 29937, 13291, 29901,\n",
       "            13, 28956,  4691,    13,  1357, 29918,   524,   353, 29871, 29945,\n",
       "            13, 28956,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,\n",
       "         11403,   278,  2286,  3342,  2038,   304,  1596,   967,   995,    13,\n",
       "            13,  2277, 29937, 10567, 29901,    13,  2158,    13,    13,  2277,\n",
       "         29937, 13291, 29901]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated instruction:\n",
      "[Iterate over string](https://www.w3schools.com/python/ref_list_iter.asp)\n",
      "\n",
      "### Instruction:\n",
      "Define a variable that contains an integer\n",
      "\n",
      "### Input:\n",
      "def\n",
      "\n",
      "### Response:\n",
      "```python\n",
      "my_int = 5\n",
      "```\n",
      "\n",
      "### Instruction:\n",
      "Use the variable defined above to print its value\n",
      "\n",
      "### Input:\n",
      "print\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Prompt:\\n{sample['description']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "# print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMpython39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
